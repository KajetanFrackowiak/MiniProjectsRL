# VDN Hyperparameters for MPE Environments
# Based on paper: "Value-Decomposition Networks For Deep Multi-Agent Reinforcement Learning"

# Training
num_episodes: 25000          # Total episodes to train
max_cycles: 25               # Max steps per episode (MPE typically 25)

# Network Architecture
learning_rate: 0.0005        # Adam optimizer learning rate (typical for VDN)
scheduler_frequency: 1000    # Steps for learning rate schedule
alpha: 0.0001                # Cosine decay alpha (minimum lr multiplier)

# Exploration
epsilon_start: 1.0           # Initial exploration rate
epsilon_end: 0.05            # Final exploration rate (5%)
epsilon_decay: 50000         # Episodes over which to decay epsilon

# Experience Replay
buffer_size: 100000          # Replay buffer capacity
batch_size: 32               # Batch size for training
gamma: 0.99                  # Discount factor
tau: 0.001                   # Soft update coefficient for target networks

# Checkpointing
checkpoint_freq: 100         # Save checkpoint every N episodes

# Notes for different environments:
# 
# simple_spread_v3:
#   - 3 agents, 18 obs dim each, 5 actions
#   - Good for basic cooperation
#   - Researchers typically train 25k episodes
#
# simple_speaker_listener_v4:
#   - 2 agents, communication task
#   - More challenging than spread
#   - Needs more exploration (higher epsilon_decay)
#
# simple_adversary_v3:
#   - 1 adversary vs 2 agents
#   - Mixed cooperative/competitive
#   - May need lower learning rate (0.0001) for stability
#
# Common variations used in literature:
# 
# VDN Paper (2017):
#   - learning_rate: 0.0001 (more conservative)
#   - batch_size: 64
#   - buffer_size: 500000
#   - tau: 0.01 (more aggressive updates)
#   - epsilon_decay: 100000 (slower decay)
#
# QMIX Paper (2018) - successor to VDN:
#   - learning_rate: 0.0005
#   - batch_size: 32
#   - buffer_size: 100000
#   - tau: 0.001 (more conservative)
#   - epsilon_decay: 50000
#   - num_episodes: 2000000 (much longer training)
